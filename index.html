<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>MissouriView</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar48"><img src="images/RS_logo.jpg" style="width:50px; height:50px" alt="" /></span>
							<h1 id="title">MissouriView</h1>
							<p>Excelling in all things geospatial</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link"><span class="icon solid fa-home">About</span></a></li>
								<li><a href="#tut" id="top-link"><span class="icon solid fa-th">Tutorials</span></a></li>
								<li><a href="#forest" id="portfolio-link"><span class="icon solid fa-th">Forest Conservation</span></a></li>
								<li><a href="#levee" id="about-link"><span class="icon solid fa-th">Water Disparity</span></a></li>
								<li><a href="#moart" id="contact-link"><span class="icon solid fa-th">Missouri as Art</span></a></li>
								<!--<li><a href="#contact" id="contact-link"><span class="icon solid fa-envelope">Contact</span></a></li>-->
							</ul>
						</nav>

				</div>

				<div class="bottom">

					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
							<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<center><h2 class="alt">Hello, Welcome to MissouriView!</h2></center>
								<p>
									This page will include tutorials that users can follow to bolster their 
									understanding of remote sensing, GIS, and Machine Learning/Artifical Intelligence, 
									all in a real-world and application-driven playground. 
									Look for yourself, a few of our tutorials are listed below. 
									Happy learning!
								</p>
							</header>
							<header>
								<p>
									The mission of the Missouri chapter of AmericaView is to promote and advance 
									the use of remote sensing technology and data across the state of Missouri. MissouriView
									is here to do just that! The geospatial ecosystem is booming in Saint Louis. One entity
									advancing geospatial aptitude is the Geospatial Institute at Saint Louis University. With a 
									cross-cutting cirriculum, GeoSLU is at the forefront of geospatial innovation. Below you  
									will encounter a few tutorials developed at the Geospatial Institute by GIS practicioners. 
									The current available tutorials are 1) Forest Conservation with AI, 2) Water Disparity 
									and Levee Management, and 3) Missouri as Art.  
									

								</p>
							</header>

							<footer>
								<center><a href="#forest" class="button scrolly">Let's Get Started!</a></center>
							</footer>

						</div>
					</section>

					<section id="forest" class="two">
						<div class="container">
							<details>
									<summary>
						  				<header>
							  				<center><h2>Tutorials</h2></center>
						  				</header>
						  				<span class="icon">üëá</span>
									</summary>
									<p>
										<a href="#forest">Forest Conservation with AI</a>><br />
										<a href="#levee">Water Disparity and Levee Management</a><br />
										<a href="#moart">Missouri as Art</a>
									</p>
					  		</details>
						</div>
					</section>

				<!-- Forest Conservation -->
					<section id="forest" class="two">
						<div class="container">

							<header>
								<center><h2>Forest Conservation with AI</h2></center>
							</header>

							<p> In this course module, students are first introduced traditional machine learning
								 including random forest (RF), support vector machine (SVM), and pixel-based deep 
								 learning using forest cover mapping in Madagascar as a case study. Then, the 
								 Convolutional Neural Network (CNN) approach is introduced, which utilizes spectral,
								 textural and spatial information from WorldView-3 VNIR and SWIR data for 
								 classification and forest cover mapping in a fully automated learning process.  
								 The CNN approach used in this lab is U-Net (similar to Object-Based Image Analysis, OBIA
								 in the remote sensing field. By completion of this lab, students can understand pros 
								 and cons of pixel-based (SVM, RF, DNN) and CNN-based OBIA; students also investigate 
								 forest change and impacts of human geography and conservation efforts on preserving forest habitats in Madagascar.
							</p>
						</div>
					</section>

					<section id="forest" class="two">
						<div class="container">
						
							<header>
								<center><h2>Area of Interest</h2></center>
							</header>	
								<center><a href="#" class="image featured"><img src="images/Picture3.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
								<p>
									The study area is the Betampona Nature Reserve (BNR), outlined by the white boundary in Figure 1 (a).  
									The BNR is located in the eastern coast of Madagascar; the surrounding areas and the nature reserve make 
									up of about 100 square km.  This is a species rich area which provides the researchers with a ‚Äúliving laboratory‚Äù 
									for the studies of human-forest interactions.<br />  
									Madagascar houses a lively and diverse ecosystem, but due to encroachment, 
									deforestation tactics (illegal logging), aggressive agricultural practices and 
									urbanization, the environment has greatly altered.  Additionally, with the presence 
									of invasive species such as Molucca Raspberry, Madagascar Cardamom, and Strawberry 
									Guava, biodiversity continues to be threatened.
								</p>
						</div>
					</section>

					<section id="forest" class="two">
						<div class="container">

                            <header>
								<center><h2>Data Processing</h2></center>

								<table>
									<tr>
										<th>File Name</th>
										<th>Description</th>
									</tr>
									<tr>
										<td>Bet_LandCover_2019.tif</td>
										<td>
											This is the ‚Äúgold-standard" land cover/use map of the study area produced 
											by object-based classification/segmentation of WV-3 imagery and significant 
											post-processing and manual investigation using field surveys and ancillary data. 
										</td>
 
									</tr>
									<tr>
										<td>Bet_LandCover_2010.tif</td>
										<td>
											Classification map of the BNR and surrounding areas in 2010, created using 
											IKONOS-2 and Hyperion image analysis. <br />
											Ghulam, A., Porton, I., Freeman, K. (2014). Detecting subcanopy invasive 
											plant species in tropical rainforest by integrating optical and microwave 
											(InSAR/PolInSAR) remote sensing data, and a decision tree algorithm. 
											ISPRS Journal of Photogrammetry and Remote Sensing, 88: 174-192.
										</td>
									</tr>
									<tr>
										<td>Polygon.shp</td>
										<td>
											This polygon represents the boundary of image patches, which will be later 
											used to train the U-Net architecture and evaluate the results. There are a 
											total 100 features in the polygon shapefile.
										</td>
									</tr>
									<tr>
										<td>WV3_19Feb2019_BNR.tif</td>
										<td>
											16 band WorldView-3 imagery in reflectance including VNIR and SWIR bands, which are 
											atmospherically and radiometrically corrected and orthorectified. <br />
											WV-3 VNIR and SWIR ground sampling distance (GSD) is 1.2 m and 3.7 m, respectively. <br />
											VNIR-SWIR bands are layer-stacked and resampled to VNIR resolution using nearest-neighbor resampling.<br />
											The SWIR bands can detect non-pigment biochemical phenomenon within plants including water, cellulose, and lignin.  
											SWIR-4 (1729.5 nm), SWIR-5 (2163.7 nm), and SWIR-8 (2329.2 nm) can identify 
											absorption phenomenon of nitrogen, cellulose, and lignin, respectively. <br />
											The data were obtained from MAXAR through priority satellite tasking during the experiment.  
										</td>
									</tr>
								</table>

								<p>
									The original ground truth data were collected in the field using GPS surveys. 
									A total of nearly 400 polygons in different sizes and shapes and point samples 
									were collected. These ground surveys were used to produce the ‚Äúgold-standard" 
									reference imagery (Bet_LandCover_2019.tif) as described in the published paper.
									For this course module, however, we generated a new set of training samples
									from the gold-standard reference imagery to simplify the implementation.<br />

									First, observe the following map:
									<center><a href="#" class="image featured"><img src="images/four_sat_img.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />

									As stated above, the WV-3 image contains 16 bands. Further details about the 16 bands 
									can be found in  Cota et al. (2021). There are a total of 11 land cover classes that 
									were created by CNN-based image segmentation.  That means all pixels in the study area 
									lready classified as some land cover/use type. The codes for each land cover class 
									are given below: <br />

									<table>
										<tr>
											<th>Land Use Classes</th>
										</tr>
										<tr>
										  <th>Int Code</th>
										  <th>Class</th>
										</tr>
										<tr>
											<td>1</td>
											<td>Mixed Forest</td>
										</tr>
										<tr>
											<td>2</td>
											<td>Evergreen Forest</td>
										</tr>
										<tr>
											<td>3</td>
											<td>Residential</td>
										</tr>
										<tr>
											<td>4</td>
											<td>Molucca Raspberry</td>
										</tr>
										<tr>
											<td>5</td>
											<td>Row Crops</td>
										</tr>
										<tr>
											<td>6</td>
											<td>Fallow</td>
										</tr>
										<tr>
											<td>7</td>
											<td>Shrubland</td>
										</tr>
										<tr>
											<td>8</td>
											<td>Open Water</td>
										</tr>
										<tr>
											<td>9</td>
											<td>Madagascar Cardamon</td>
										</tr>
										<tr>
											<td>10</td>
											<td>Grassland</td>
										</tr>
										<tr>
											<td>11</td>
											<td>Guava</td>
										</tr>
									</table>

									U-NET requires image patches or image chips as input data. To create this data for 
									this tutorial, we generated a polygon shapefile, named ‚Äòpolygon.shp‚Äù, based on the 
									gold-standard land cover/use map. This polygon represents the boundary of image 
									patches, which will be later used to train the U-Net architecture and evaluate the 
									results. There are a total of 100 features in the polygon shapefile. The steps to 
									generate these training samples are below. <br />
									1.	Create a set of randomly generated points using ArcGIS Pro. To do this, 
									open ArcToolbox and open the Data Management 
									Tools > Feature Class > Create random points tool.<br />
									2.	Create a circular buffer around those points, set buffer size to 192 meters.<br />
									3.	Convert the circle to a square-shaped polygon by using ‚ÄúMinimum Bounding Geometry‚Äù 
									tool and use ‚ÄúEnvelope‚Äù as the geometry type

									<center><a href="#" class="image featured"><img src="images/Polygons.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />
								</p>
							</header>
						</div>
					</section>
					<section id="forest" class="two">
						<div class="container">
							<header>
								<center><h2>Objectives</h2></center>
								<p>Please go through all the provided DEMO notebooks:<br /> 
								
									‚Ä¢UNET_part1A_preprocessing_DEMO.ipynb<br />
										o	Demonstrate the preprocessing steps.<br />
										o	After preprocessing, save the train and test set in npy format.<br />
									‚Ä¢UNET_part1B_training_DEMO.ipynb<br />
										o	Perform the training of UNET.<br />
										o	Save the best model.<br />
									‚Ä¢UNET_part1C_applying_model_to_map_DEMO.ipynb<br />
										o	Apply the model to the whole image.<br />
										o	Divide the image into similar size image patches.<br />
										o	Then apply model to each patch.<br />
									</p>
							</header>
						</div>
					</section>
					<section id="forest" class="two">
						<div class="container">
							<header>
								<center><h2>Assignment</h2></center>
								<p>Write a report (no more than 4 pages) about the methods and results from this lab. 
									You can rely on the Cota et al. (2021) publication for reference, but note that the
									goal for this report is how the U-Net worked and how you implemented it. Focus on how 
									each segment of code worked and what are the 5 key skills you learned about CNN and 
									U-NET. You can include figures but do not fill up the whole report with just figures. 
									Here is a template you can follow:<br />
	
									1.	Introduction: Talk about the background behind the task. Why are you doing this? What are you expecting from the model? The objectives.<br />
									2.	Methods: (Try to create a short figure showing the overall workflow)<br />
										a.	Preprocessing: How did you preprocess the data. Try to briefly describe the tasks of the codes provided in the notebook<br />
										b.	U-NET Training: How many parameters were there? What hyperparameter values you used? What loss function and optimizer you used and why?<br />
										c.	Applying to Image: How did you apply the model to the whole image?<br />
									3.	Results: What is the overall accuracy? Show the confusion matrix, kappa scores and other metrics you want to show.<br />
									4.	Discussion: is this a segmentation or classification approach? What are the advantages and disadvantages between the two? Did U-NET utilize spectral information? What about texture and spatial patterns?<br />
									5.	Conclusion: What did you achieve? What are the possible next steps that can improve the results?<br />
									
									Use Time New Roman / Arial / Calibri with 11 pt font size. Use default line 
									spacing (Multiple, 1.08). Letter page.
								</p>
							</header>
						</div>
					</section>
					<section id="forest" class="two">
						<div class="container">
							<header>
								<center><h2>Grading Scale</h2></center>
							</header>
							<p>
								1.	Run all the codes correctly and generate figures in the notebook (50%)<br />
								2.	Write down the report (50% Broken down below)<br />
									a.	Introduction (5%)<br />
                                            i. Appropriate background and problem statement<br />
                                            ii.Clear objective<br />
									b.	Methods (20%)<br />
                                            i. Clear picture about the methods<br />
                                           	ii. Explain the major functions<br />
									c.	Results (10%)<br />
                                            i. Evaluation metrics for test set<br />
                                            ii. Train-validation loss curve<br />
									d.	Discussion (10%)<br />
                                            i. Discuss the results<br />
                                            ii.Difference between segmentation and classification<br />
									e.	Conclusion (5%)
							</p>
						</div>
						<center><a href="#" class="image featured"><img src="images/natcapital.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
					</section>
					
				<!-- Lets Talk About Levee's! -->
					<section id="levee" class="three">
						<div class="container">

							<header>
								<center><h2>Water Disparity and Levee Management</h2></center>
							</header>

							<p> 
								Coming Soon
							</p>

						</div>
					</section>
					<section id="levee" class="three">
						<div class="container">
							<header>
								<center><h2>Data</h2></center>
							</header>

							<p> 
								Coming Soon!
							</p>
						</div>
					</section>

				<!--MO as Art Tutorial Spot-->	
					<section id="moart" class="one dark cover">
						<div class="container">
							<header>
								<center><h2>Missouri as Art Tutorial Coming Soon!</h2></center>
							</header>
							<p></p>
						</div>
					</section>

				
				<!--Potential Holding place-->

				<!-- Contact -->
					<!--<section id="contact" class="four">
						<div class="container">

							<header>
								<center><h2>Contact and Submission</h2></center>
							</header>

							<p>Please feel free to contact us with any questions that may arise.
							   Additionally, you can also submit your work in this field.  We thank you
							   for visiting this page and we hope you learned something that you can take
							   with you into your professional and academic careers. 
							</p>

							<form method="post" action="#">
								<div class="row">
									<div class="col-6 col-12-mobile"><input type="text" name="name" placeholder="Name" /></div>
									<div class="col-6 col-12-mobile"><input type="text" name="email" placeholder="Email" /></div>
									<div class="col-12">
										<textarea name="message" placeholder="Message"></textarea>
									</div>
									<div class="col-12">
										<input type="submit" value="Send Message" />
									</div>
								</div>
							</form>

						</div>
					</section>-->

			</div>

			<section>
				<center><header><h2>This is a Test</h2></header></center>
				<details>
					<summary>
					  What is the meaning of life?
					  <span class="icon">^</span>
					</summary>
					<p>
					  This should open from the collapsible window!
					</p>
				  </details>
			</section>
		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; SLU RS Lab. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
