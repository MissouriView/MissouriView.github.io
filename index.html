<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>MissouriView</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar48"><img src="images/RS_logo.jpg" style="width:50px; height:50px" alt="" /></span>
							<h1 id="title">MissouriView</h1>
							<p>Excelling in all things geospatial</p>
						</div>
					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link">About</span></a></li>
								<li><a href="#part" id="portfolio-link">Partners</span></a></li>	
								<li><a href="#tut" id="portfolio-link"><div class="dropdown">Tutorials
									<div class="dropdown-content">
										<a href="#forest" id="portfolio-link">Forest Conservation</a>
										<a href="#levee" id="portfolio-link">Water Disparity</a>
										<a href="#art" id="portfolio-link">Missouri as Art</a>
									</div>
								</li>
								
								<!--<li><a href="#forest" id="portfolio-link"><span class="icon solid fa-th">Forest Conservation</span></a></li>
								<li><a href="#levee" id="portfolio-link"><span class="icon solid fa-th">Water Disparity</span></a></li>
								<li><a href="#art" id="portfolio-link"><span class="icon solid fa-th">Missouri as Art</span></a></li>-->
							</ul>
						</nav>

				</div>

				<div class="bottom">
			
					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="https://github.com/MissouriView" target= "_blank" class="icon brands fa-github"><span class="label" >Github</span></a></li>
							<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<center><h2 class="alt">Hello, Welcome to MissouriView!</h2></center>
								<p>
									This page will include tutorials that users can follow to bolster their 
									understanding of remote sensing, GIS, and Machine Learning/Artifical Intelligence, 
									all in a real-world and application-driven playground. 
									Look for yourself, a few of our tutorials are listed below. 
									Happy learning!
								</p>
							</header>
							<header>
								<p>
									The mission of the Missouri chapter of AmericaView is to promote and advance 
									the use of remote sensing technology and data across the state of Missouri. MissouriView
									is here to do just that! The geospatial ecosystem is booming in Saint Louis. One entity
									advancing geospatial aptitude is the Geospatial Institute at Saint Louis University. With a 
									cross-cutting cirriculum, GeoSLU is at the forefront of geospatial innovation. Below you  
									will encounter a few tutorials developed at the Geospatial Institute by GIS practicioners. 
									The current available tutorials are 1) Forest Conservation with AI, 2) Water Disparity 
									and Levee Management, and 3) Missouri as Art.  
									

								</p>
							</header>

							<footer>
								<center><a href="#part" class="button scrolly">Let's Get Started!</a></center>
							</footer>
						</div>
					</section>	
					
						<!--This is for partners-->
							<section id="part" class="two">
								<div class="container">
									<details>
										<summary>
											<header>
												<center><h2>Partners</h2></center>
											</header>
										<span class="icon">↓</span>
										</summary>
										<p>
		
											<header>
											<center><h2 class="alt">Our Partners!</h2></center>

											<a href="https://www.downtowntrex.org/" target= "_blank"><img src="images/T-REX-logo.png" style="float: left; height: 4em; margin-right: 4%; margin-bottom: 3em;"></a>
											<a href="https://www.umsl.edu/" target= "_blank"><img src="images/umsl.png" style="float: left; height: 3em; width: 28%; margin-right: 4%; margin-bottom: 3em;">	
											<a href="https://wustl.edu/"target= "_blank"><img src="images/WashU.png" style="float: left; height: 4em; margin-right: 4%; margin-bottom: 3em;"></a>

											<a href="https://www.cortexstl.com/" target= "_blank"><img src="images/cortex_logo.png" style="float: left; height: 2em; width: 28%; margin-right: 3%; margin-bottom: 3em;"></a>
											<a href="https://www.hssu.edu/" target= "_blank"><img src="images/HSSU.png" style="float: left; height: 3em; width: 30% ;margin-right: 4%; margin-bottom: 3em;"></a>
											<a href="https://missouri.edu/" target= "_blank"><img src="images/Mizzou.png" style="float: left; height: 4em; margin-right: 3%; margin-bottom: 3em"></a>

											<a href="https://www.missouristate.edu/" target= "_blank"><img src="images/MOState.png" style="float: left; height: 6em;  margin-right: 2%; margin-bottom: 1em; margin-top: 2em"></a>
											<a href="https://www.nwmissouri.edu/" target= "_blank"><img src="images/NorthwestMOUNI.png" style="float: left; height: 6em; margin-right: 2%; margin-bottom: 1em; margin-top: 2em"></a>
											<a href="https://www.slu.edu/" target= "_blank"><img src="images/slu.png" style="float: left; height: 9em; margin-left: 2%; margin-right: 2%; margin-bottom: 2em"></a>
											<a href="https://www.siue.edu/" target= "_blank"><img src="images/SIUE.png" style="float: left; height: 3em; width: 30%; margin-right: 2%; margin-bottom: 1em; margin-top: 3em"></a>
											

											<p style="clear: both;"></p>

										</p>

									</header>
								</div>
							</section>
									
								<!-- Forest Conservation -->
										<section id="forest" class="two">
											<div class="container">
												<details>
													<summary>
														<header>
															<center><h2>Forest Conservation with AI</h2></center>
														</header>
													<span class="icon">↓</span>
													</summary>
													<!--Everything about module needs to go in between these two p's for dropdown!-->		
													<p>
														
														<p> In this course module, students are first introduced traditional machine learning
								 							including random forest (RF), support vector machine (SVM), and pixel-based deep 
								 							learning using forest cover mapping in Madagascar as a case study. Then, the 
								 							Convolutional Neural Network (CNN) approach is introduced, which utilizes spectral,
								 							textural and spatial information from WorldView-3 VNIR and SWIR data for 
								 							classification and forest cover mapping in a fully automated learning process.  
								 							The CNN approach used in this lab is U-Net (similar to Object-Based Image Analysis, OBIA
								 							in the remote sensing field. By completion of this lab, students can understand pros 
								 							and cons of pixel-based (SVM, RF, DNN) and CNN-based OBIA; students also investigate 
								 							forest change and impacts of human geography and conservation efforts on preserving 
															forest habitats in Madagascar. You can access the data for this tutorial on our <a href="https://github.com/MissouriView/Forest-Conservation-with-AI" target= "_blank">github page</a>.
														</p>
					

														<header>
															<center><h2>Area of Interest</h2></center>
														</header>	
															<center><a href="#" class="image featured"><img src="images/Picture3.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
														<p>
															The study area is the Betampona Nature Reserve (BNR), outlined by the white boundary in Figure 1 (a).  
															The BNR is located in the eastern coast of Madagascar; the surrounding areas and the nature reserve make 
															up of about 100 square km.  This is a species rich area which provides the researchers with a “living laboratory” 
															for the studies of human-forest interactions.<br />  
															Madagascar houses a lively and diverse ecosystem, but due to encroachment, 
															deforestation tactics (illegal logging), aggressive agricultural practices and 
															urbanization, the environment has greatly altered.  Additionally, with the presence 
															of invasive species such as Molucca Raspberry, Madagascar Cardamom, and Strawberry 
															Guava, biodiversity continues to be threatened.
														</p>

                            							<header>
															<center><h2>Data Processing</h2></center>
														</header>

														<head>
															<style>
																table,
																th,
																td {
																	border: 1px solid black;
																}
																table.center {
																	  margin-left: auto;
																	 margin-right: auto;
																}
															</style>
														</head>
														<body>
															<table style="width:100%" class="center">
																<tr>
																	<th>File Name</th>
																	<th>Description</th>
																</tr>
																<tr>
																	<td>Bet_LandCover_2019.tif</td>
																	<td>
																		This is the “gold-standard" land cover/use map of the study area produced 
																		by object-based classification/segmentation of WV-3 imagery and significant 
																		post-processing and manual investigation using field surveys and ancillary data. 
																	</td>
 
																</tr>
																<tr>
																	<td>Bet_LandCover_2010.tif</td>
																	<td>
																		Classification map of the BNR and surrounding areas in 2010, created using 
																		IKONOS-2 and Hyperion image analysis. <br />
																		Ghulam, A., Porton, I., Freeman, K. (2014). Detecting subcanopy invasive 
																		plant species in tropical rainforest by integrating optical and microwave 
																		(InSAR/PolInSAR) remote sensing data, and a decision tree algorithm. 
																		ISPRS Journal of Photogrammetry and Remote Sensing, 88: 174-192.
																	</td>
																</tr>
																<tr>
																	<td>Polygon.shp</td>
																	<td>
																		This polygon represents the boundary of image patches, which will be later 
																		used to train the U-Net architecture and evaluate the results. There are a 
																		total 100 features in the polygon shapefile.
																	</td>
																</tr>
																<tr>
																	<td>WV3_19Feb2019_BNR.tif</td>
																	<td>
																		16 band WorldView-3 imagery in reflectance including VNIR and SWIR bands, which are 
																		atmospherically and radiometrically corrected and orthorectified. <br />
																		WV-3 VNIR and SWIR ground sampling distance (GSD) is 1.2 m and 3.7 m, respectively. <br />
																		VNIR-SWIR bands are layer-stacked and resampled to VNIR resolution using nearest-neighbor resampling.<br />
																		The SWIR bands can detect non-pigment biochemical phenomenon within plants including water, cellulose, and lignin.  
																		SWIR-4 (1729.5 nm), SWIR-5 (2163.7 nm), and SWIR-8 (2329.2 nm) can identify 
																		absorption phenomenon of nitrogen, cellulose, and lignin, respectively. <br />
																		The data were obtained from MAXAR through priority satellite tasking during the experiment.  
																	</td>
																</tr>
															</table>
														</body>

															<p>
																The original ground truth data were collected in the field using GPS surveys. 
																A total of nearly 400 polygons in different sizes and shapes and point samples 
																were collected. These ground surveys were used to produce the “gold-standard" 
																reference imagery (Bet_LandCover_2019.tif) as described in the published paper.
																For this course module, however, we generated a new set of training samples
																from the gold-standard reference imagery to simplify the implementation.<br />

																First, observe the following map:
																<center><a href="#" class="image featured"><img src="images/four_sat_img.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />

																As stated above, the WV-3 image contains 16 bands. Further details about the 16 bands 
																can be found in  Cota et al. (2021). There are a total of 11 land cover classes that 
																were created by CNN-based image segmentation.  That means all pixels in the study area 
																already classified as some land cover/use type. The codes for each land cover class 
																are given below: <br />

																<head>
																	<style>
																		table,
																		th,
																		td {
																			border: 1px solid black;
																		}
																		table.center {
  																			margin-left: auto;
 																			margin-right: auto;
																		}
																	</style>
																</head>
																<body>
																	<table style="width:70%" class="center">
																		<tr>
										  									<th>Int Code</th>
										  									<th>Class</th>
																		</tr>
																		<tr>
																			<td>1</td>
																			<td>Mixed Forest</td>
																		</tr>
																		<tr>
																			<td>2</td>
																			<td>Evergreen Forest</td>
																		</tr>
																		<tr>
																			<td>3</td>
																			<td>Residential</td>
																		</tr>
																		<tr>
																			<td>4</td>
																			<td>Molucca Raspberry</td>
																		</tr>
																		<tr>
																			<td>5</td>
																			<td>Row Crops</td>
																		</tr>
																		<tr>
																			<td>6</td>
																			<td>Fallow</td>
																		</tr>
																		<tr>
																			<td>7</td>
																			<td>Shrubland</td>
																		</tr>
																		<tr>
																			<td>8</td>
																			<td>Open Water</td>
																		</tr>
																		<tr>
																			<td>9</td>
																			<td>Madagascar Cardamon</td>
																		</tr>
																		<tr>
																			<td>10</td>
																			<td>Grassland</td>
																		</tr>
																		<tr>
																			<td>11</td>
																			<td>Guava</td>
																		</tr>
																	</table>
																</body>
															</p>
															<p>
															U-NET requires image patches or image chips as input data. To create this data for 
															this tutorial, we generated a polygon shapefile, named ‘polygon.shp”, based on the 
															gold-standard land cover/use map. This polygon represents the boundary of image 
															patches, which will be later used to train the U-Net architecture and evaluate the 
															results. There are a total of 100 features in the polygon shapefile. The steps to 
															generate these training samples are below. <br />
															1.	Create a set of randomly generated points using ArcGIS Pro. To do this, 
															open ArcToolbox and open the Data Management 
															Tools > Feature Class > Create random points tool.<br />
																2.	Create a circular buffer around those points, set buffer size to 192 meters.<br />
															3.	Convert the circle to a square-shaped polygon by using “Minimum Bounding Geometry” 
															tool and use “Envelope” as the geometry type

															<center><a href="#" class="image featured"><img src="images/Polygons.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />
														</p>
					
														<header>
															<center><h2>Steps</h2></center>
															<p> Please go through all the provided DEMO notebooks:<br /> 
								
																•UNET_part1A_preprocessing_DEMO.ipynb<br />
																o	Demonstrate the preprocessing steps.<br />
																o	After preprocessing, save the train and test set in npy format.<br />
																•UNET_part1B_training_DEMO.ipynb<br />
																o	Perform the training of UNET.<br />
																o	Save the best model.<br />
																•UNET_part1C_applying_model_to_map_DEMO.ipynb<br />
																o	Apply the model to the whole image.<br />
																o	Divide the image into similar size image patches.<br />
																o	Then apply model to each patch.<br />
															</p>
														</header>
			
														<header>
															<center><h2>Assignment</h2></center>
															<p>	
																Write a report (no more than 4 pages) about the methods and results from this lab. 
																You can rely on the Cota et al. (2021) publication for reference, but note that the
																goal for this report is how the U-Net worked and how you implemented it. Focus on how 
																each segment of code worked and what are the 5 key skills you learned about CNN and 
																U-NET. You can include figures but do not fill up the whole report with just figures. 
																Here is a template you can follow:<br />
	
																1.	Introduction: Talk about the background behind the task. Why are you doing this? What are you expecting from the model? The objectives.<br />
																2.	Methods: (Try to create a short figure showing the overall workflow)<br />
																	a.	Preprocessing: How did you preprocess the data. Try to briefly describe the tasks of the codes provided in the notebook<br />
																	b.	U-NET Training: How many parameters were there? What hyperparameter values you used? What loss function and optimizer you used and why?<br />
																	c.	Applying to Image: How did you apply the model to the whole image?<br />
																3.	Results: What is the overall accuracy? Show the confusion matrix, kappa scores and other metrics you want to show.<br />
																4.	Discussion: is this a segmentation or classification approach? What are the advantages and disadvantages between the two? Did U-NET utilize spectral information? What about texture and spatial patterns?<br />
																5.	Conclusion: What did you achieve? What are the possible next steps that can improve the results?<br />
									
																Use Time New Roman / Arial / Calibri with 11 pt font size. Use default line 
																spacing (Multiple, 1.08). Letter page.
															</p>
														</header>
					
														<header>
															<center><h2>Grading Scale</h2></center>
														</header>
															<p>
																1.	Run all the codes correctly and generate figures in the notebook (50%)<br />
																2.	Write down the report (50% Broken down below)<br />
																	a.	Introduction (5%)<br />
                                            							i. Appropriate background and problem statement<br />
                                            							ii.Clear objective<br />
																	b.	Methods (20%)<br />
                                            							i. Clear picture about the methods<br />
                                           								ii. Explain the major functions<br />
																	c.	Results (10%)<br />
                                            							i. Evaluation metrics for test set<br />
                                            							ii. Train-validation loss curve<br />
																	d.	Discussion (10%)<br />
                                            							i. Discuss the results<br />
                                           	 							ii.Difference between segmentation and classification<br />
																	e.	Conclusion (5%)
															</p>
								
														<center><a href="#" class="image featured"><img src="images/natcapital.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
													</p>
												</details>
											</div>
										</section>
										

									<!-- Lets Talk About Levee's! -->
										<section id="levee" class="two">
											<div class="container">
												<details>
													<summary>
									  					<header>
										  					<center><h2>Water Disparity and Levee Management</h2></center>
									  					</header>
									  				<span class="icon">↓</span>
													</summary>
													<!--Everything about module needs to go in between these two p's for dropdown!-->
													<p> 
														This project uses LiDAR and Photogrammetry data for levee managmeent
														An essential aspect of large rivers is the human need to control them. Installing 
														levees is one way that society has tried to control flooding on large rivers. As levees 
														age, they need to be managed and upkept. In areas where levees are managed by different groups, 
														instances of inconsistent management can sometimes lead to nearby levees being updated while others 
														are forgotten. One case of inconsistent management, and a subsequent levee failure, is the Len 
														Small Levee on the Mississippi River, just north of it’s confluence with the Ohio River in a stretch 
														of river known as the Dogtooth Bend. The Len Small Levee is a farmer-owned levee currently being eroded
														by the Mississippi River as it forms a chute across farm lands. Now, residents living in the area must 
														travel by boat if they wish to leave the newly formed island. You can access the data for this tutorial on our <a href="https://github.com/MissouriView/Water-Disparity-and-Levee-Management" target= "_blank">github page</a>.<br />
									
														<header>
															<center><h2>Theoretical Background</h2></center>
														</header>	
															<p>
																LiDAR stands for light detection and ranging.This is a relatively new remote sensing technology 
																that allows users to collect very dense points samples of features in 3D. Using these dense points, we 
																can process them into varying visualizations that describe different features  on the Earth’s Surface. 
																In this lab you will be explain the processing step to converting LiDAR data a Digital Elevation Model of 
																the average elevation of the Levee and Colorize LSA Points
																
																Levees can be naturally constructed by sediment deposited or an embankment built alongside a river to prevent 
																the overflow of a river system. Studying the levee systems and river systems along the Mississippi River 
																is important. The role of the Mississippi River is crucial to nearby land owners, residents and 
																agricultural settlements. Understanding the relationship and change of the Mississippi River and Levee has 
																an influence on the settlements near the area. Thus make it an important area of interest to study and 
																monitor. 
															</p>

														<header>
															<center><h2>Methods</h2></center>
														</header>
															<p>
																The Remote Sensing Lab equipment provided:<br /> <!--fix this section-->
																	Velodyne HDL-32<br />
																	Sony A7R II w 15mm aspherical lens - 42MP Full-Frame Exmor R BSI CMOS Sensor<br />
																	Upgraded GPS and GNSS IMU-27
															</p>
														<center><a href="#" class="image featured"><img src="images/one.jpg" style= "width: 300px; height:auto" display="block" alt="" class="center" /></a></center>
														<center><a href="#" class="image featured"><img src="images/two.jpg" style= "width: 300px; height:auto" display="block" alt="" class="center" /></a></center>
															<p>
																The Velodyne LiDAR system can swing 360, but since it is pointing vertically downward the set up swings 
																180 and the mirror in a 45 degree angle. The system is measuring the distance between itself and a target, 
																in this case the area interesting, levee river system. The light emitted from the LiDAR system travels to 
																a target and reflects off of its surface and comes back to its source. Represented with a simple physic 
																equation D=V*T, accounting for the speed of light emitted from the LiDAR system and knowing the time of 
																flight. Distance to the target is calculated. In addition, the LiDAR system has a built-in gps system and 
																GNSS IMU-27 is set up to give us position and orientation of the sensor; known XYZ coordinates of the 
																reflective surface can be measured and represented by a point.  Using Phoenix Software we can create a 
																flight path and repeat this process to build a complex map composed of all the points that the LiDAR 
																system collected.

																Digital elevation models are 3D computer graphics representation of elevation data to represent terrain. 
																Applications can vary from landscape modeling, city modeling, visualization application, land-use studies, 
																geological applications. For this lab our purpose is to create a DEM and find the average elevation of 
																the levee system for flood and drainage modeling.

																<center><a href="#" class="image featured"><img src="images/three.jpg" style= "width: 350px; height:auto" display="block" alt="" class="center" /></a></center>
															</p>

														<header>
															<center><h2>Creating a Digital Elevation Model and Calculating Average Elevation</h2></center>
														</header>	
															<p>
																To organize ourselves, lets make two separate folders <br />
																	Folder 1): Lab#<br />
																		-Lab#_Name.aprx<br />
																	Folder2): Steps<br />
																		1_LAS_Dataset<br />
																		2_Raster<br />
																		3_GeneratePoints<br />
																		4_ExtractValue_Point<br />
																		5_AvgElevation<br />

																<h1>Step 1 LAS Dataset</h1>

																Within the geoprocessing tool, search up Create LAS Dataset. Use the given LAS Laser Point 
																Files as inputs. <br />

																Select the correct pathway to the output step folder, <br />
																Input: LAS_Dataset<br />
																Output: 1_LAS_Dataset and name the feature <br />

																Coordinates: [XY: WGS 1984 UTM Zone 16N, Z: NAVD 1988]

																<h1>Step 2 LAS Dataset to Raster</h1>
																Input LAS Dataset: LAS dataset<br />
																Output: 2_Raster and name the feature 

																Try out different interpolation types and parameters<br />
																Binning or Triangulation <br />
																Cell assignment: [Average, IDW]<br />
																Void: [None, Linear, SImple, Natural Neighbor]<br />
																Sampling Size: [0.05, 0.2, 0.3, 0.5, 1.0] (cm)

																<h1>Step 3 Digitize Levee Line</h1>
																In the Catalog Section, find your geodatabase, right click and create feature class<br />
																1/6 Name your feature class<br />
		   														Select the Line as your Feature Class Type<br />
		    													Add output dataset to current map should be the only box checked<br />
																2/6-6/6 Next and Finish <br />
																After finish creating the Levee feature class, it will appear on the Content.

																To digitize the Levee line, go to the Edit Ribbon and find the create icon in the 
																feature box. From there the Create Feature Panel will appear on the right side 
																where you start drawing the line on top of the Levee. Trace as many lines need, 
																but follow the Levee (white pixels) The results should be below, adjust symbology 
																to one’s liking.

																<center><a href="#" class="image featured"><img src="images/six.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
																
																<h1>Step 4 Generate Points Along Lines</h1>

																Input Feature: Select Levee Feature Line Class<br />
																Output Feature Class Make a 3_GeneratePoints  and name the feature <br />
																Point Placement: By Distance<br />
																Distance: 20 meters<br />
																Run, Point will appear
																<center><a href="#" class="image featured"><img src="images/seven.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>

																<h1>Step 5 Extract Values From Points</h1>
																Use the Extract Values to Points tool to get the elevation at each point along the Levee Line. Output to 
																4_ExtractValue_Point and name the feature. A new set of points are created, change the symbology. Once the new 
																points have been created, right click the points in the Content Panel and view the Attribute Table. We can see 
																the Raster Values they should be around 72 units.

																<h1>Step 6 Summarize the Mean Elevation</h1>
																Right click the Attribute Table and select Summarize. Summary Statistics Panel will pop up. From there input your Extracted Levee 
																Value Points. Output to the Step 5_AvgElevation and name the feature. The Field desired for a statistical analysis is the RasterValu, 
																Select that and Mean. Hit Ok. A New Standalone Table is created, View and compare the values.
															</p>

														<header>
															<h2>Final Ouput</h2>
														</header>

															<p>
																The final output should look similar to this figure below. 
																<center><a href="#" class="image featured"><img src="images/10_final.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
															</p>
														<header>
															<h2>Deliverables</h2>
														</header>
															<p>
																Contents shoule be clearly labeled and placed in a zip file. <br />
																File name should be: GIS####_Lab#_FULLNAME<br />
																Lab Folder: with ArcGIS Project File<br />
																Step Folder steps 1-5 (creating las data, raster, generate points, extracting value & average elevation) <br />
																Word Document of the Questions and Answer<br />

																<h1>Questions:</h1>
																1. When creating your raster file, some of the area might not be smoothed out. What contributes to the noise or
																lack of smoothness? In addition, what interpolation types and parameters made it cleaner? <br />

																2. What is the Average Levee Elevation? Specific a range, doesn’t have to be exact <br />

																3. What features can be observed? <br />

															</p>
													</p>
												</details>
											</div>
										</section>

										<!--MO as Art Tutorial Spot-->	
										<section id="art" class="two">
											<div class="container">
												<details>
													<summary>
														<header>
															<center><h2>Missouri as Art</h2></center>
														</header>
														<span class="icon">↓</span>
													</summary>
														<!--Everything about module needs to go in between these two p's for dropdown!-->
														
														<p>
															Missouri is a naturally beautiful state, and one of 
															the most stunning ways to visualize Missouri’s beauty is by using different 
															multispectral band combinations to optimize the visualization of remote sensing data 
															to highlight different features on the ground. The Missouri as Art portion of MissouriView’s 
															mission makes remote sensing data more visually intriguing and artistic for building interest 
															from K-12 communities, while introducing students to applied concepts in remote sensing. 
															Here, students are introduced to the Electromagnetic Spectrum, the concepts of remote sensing 
															bands, how different bands are visualized, and how different combinations of bands can 
															highlight features of different types (i.e. water, agriculture, mineral exploration and 
															extraction, etc...).Power point material provides introduction to remote sensing with ample 
															examples of imagery collected in Missouri. With intuitive combination of satellite imagery, 
															the material highlight distinct Missouri landscape, ideal training material for K-16. You can access the full powerpoint on our <a href="https://github.com/MissouriView/Missouri-as-Art" target= "_blank">github page</a>.<br />
														

															<header>
																<h2>Slide Show</h2>
															</header>
																<!-- Container for the image gallery -->
																<div class="container">

																	<!-- Full-width images with number text -->
																	<div class="mySlides">
	  																	<div class="numbertext">1 / 6</div>
																			<img src="images/slide1.jpg" style="width:100%">
																	</div>
  
																	<div class="mySlides">
	  																	<div class="numbertext">2 / 6</div>
																			<img src="images/slide2.jpg" style="width:100%">
																	</div>
  
																	<div class="mySlides">
	 																 	<div class="numbertext">3 / 6</div>
																			<img src="images/slide3.jpg" style="width:100%">
																	</div>
  
																	<div class="mySlides">
	 																 	<div class="numbertext">4 / 6</div>
																			<img src="images/slide4.jpg" style="width:100%">
																	</div>
  
																	<div class="mySlides">
	 																 	<div class="numbertext">5 / 6</div>
																			<img src="images/slide5.jpg" style="width:100%">
																	</div>
  
																	<div class="mySlides">
	  																	<div class="numbertext">6 / 6</div>
																			<img src="images/slide6.jpg" style="width:100%">
																	</div>
  
																	<!-- Next and previous buttons -->
																	<a class="prev" onclick="plusSlides(-1)">&#10094;</a>
																	<a class="next" onclick="plusSlides(1)">&#10095;</a>
  
																	<!-- Image text -->
																	<div class="caption-container">
	  																	<p id="caption"></p>
																	</div>
  
																	<!-- Thumbnail images -->
																	<div class="row">
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide1.jpg" style="width:100%" onclick="currentSlide(1)" alt="The Woods">
	  																	</div>
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide2.jpg" style="width:100%" onclick="currentSlide(2)" alt="Cinque Terre">
	  																	</div>
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide3.jpg" style="width:100%" onclick="currentSlide(3)" alt="Mountains and fjords">
	  																	</div>
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide4.jpg" style="width:100%" onclick="currentSlide(4)" alt="Northern Lights">
	  																	</div>
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide5.jpg" style="width:100%" onclick="currentSlide(5)" alt="Nature and sunrise">
	  																	</div>
	  																	<div class="column">
																			<img class="demo cursor" src="images/slide6.jpg" style="width:100%" onclick="currentSlide(6)" alt="Snowy Mountains">
	  																	</div>
																	</div>
  																</div>
														</p>
												</details>
											</div>
										</section>
		
				<!--Potential Holding place-->

				
			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; SLU RS Lab. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
