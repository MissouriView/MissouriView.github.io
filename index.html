<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>MissouriView</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar48"><img src="images/RS_logo.jpg" style="width:50px; height:50px" alt="" /></span>
							<h1 id="title">MissouriView</h1>
							<p>Excelling in all things geospatial</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link"><span class="icon solid fa-home">About</span></a></li>
								<li><a href="#part" id="portfolio-link"><span class="icon solid fa-th">Partners</span></a></li>	
								<li>
									<a
										href="#tut" id="portfolio-link"> 
											<div class="dropdown">tutorials>
												<button class="dropbtn">Dropdown
													<i class="fa fa-caret-down"></i>
												</button>
												<div class="dropdown-content">
													<li><a href="#forest" id="portfolio-link"><span class="icon solid fa-th">Forest Conservation</span></a></li>
													<li><a href="#levee" id="portfolio-link"><span class="icon solid fa-th">Water Disparity</span></a></li>
													<li><a href="#art" id="portfolio-link"><span class="icon solid fa-th">Missouri as Art</span></a></li>
												</div>
											</div>
									</a>
								</li>
								
								<!--<li><a href="#forest" id="portfolio-link"><span class="icon solid fa-th">Forest Conservation</span></a></li>
								<li><a href="#levee" id="portfolio-link"><span class="icon solid fa-th">Water Disparity</span></a></li>
								<li><a href="#art" id="portfolio-link"><span class="icon solid fa-th">Missouri as Art</span></a></li>-->
							</ul>
						</nav>

				</div>

				<div class="bottom">
			
					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="https://github.com/MissouriView" target= "_blank" class="icon brands fa-github"><span class="label" >Github</span></a></li>
							<li><a href="#" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<center><h2 class="alt">Hello, Welcome to MissouriView!</h2></center>
								<p>
									This page will include tutorials that users can follow to bolster their 
									understanding of remote sensing, GIS, and Machine Learning/Artifical Intelligence, 
									all in a real-world and application-driven playground. 
									Look for yourself, a few of our tutorials are listed below. 
									Happy learning!
								</p>
							</header>
							<header>
								<p>
									The mission of the Missouri chapter of AmericaView is to promote and advance 
									the use of remote sensing technology and data across the state of Missouri. MissouriView
									is here to do just that! The geospatial ecosystem is booming in Saint Louis. One entity
									advancing geospatial aptitude is the Geospatial Institute at Saint Louis University. With a 
									cross-cutting cirriculum, GeoSLU is at the forefront of geospatial innovation. Below you  
									will encounter a few tutorials developed at the Geospatial Institute by GIS practicioners. 
									The current available tutorials are 1) Forest Conservation with AI, 2) Water Disparity 
									and Levee Management, and 3) Missouri as Art.  
									

								</p>
							</header>

							<footer>
								<center><a href="#part" class="button scrolly">Let's Get Started!</a></center>
							</footer>
						</div>
					</section>	
					
						<!--This is for partners-->
							<section id="part" class="two">
								<div class="container">
		
									<header>
										<center><h2 class="alt">Our Partners!</h2></center>

										<p>Coming Soon!</p>
									</header>
								</div>
							</section>
									
								<!-- Forest Conservation -->
										<section id="forest" class="two">
											<div class="container">
												<details>
													<summary>
														<header>
															<center><h2>Forest Conservation with AI</h2></center>
														</header>
													<span class="icon">↓</span>
													</summary>
													<!--Everything about module needs to go in between these two p's for dropdown!-->		
													<p>
														<header>
															<h3><a href="https://github.com/MissouriView/Forest-Conservation-with-AI" target= "_blank">Link</a> to Github page</h3>
														</header>

														<p> In this course module, students are first introduced traditional machine learning
								 							including random forest (RF), support vector machine (SVM), and pixel-based deep 
								 							learning using forest cover mapping in Madagascar as a case study. Then, the 
								 							Convolutional Neural Network (CNN) approach is introduced, which utilizes spectral,
								 							textural and spatial information from WorldView-3 VNIR and SWIR data for 
								 							classification and forest cover mapping in a fully automated learning process.  
								 							The CNN approach used in this lab is U-Net (similar to Object-Based Image Analysis, OBIA
								 							in the remote sensing field. By completion of this lab, students can understand pros 
								 							and cons of pixel-based (SVM, RF, DNN) and CNN-based OBIA; students also investigate 
								 							forest change and impacts of human geography and conservation efforts on preserving forest habitats in Madagascar.
														</p>
					

														<header>
															<center><h2>Area of Interest</h2></center>
														</header>	
															<center><a href="#" class="image featured"><img src="images/Picture3.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
														<p>
															The study area is the Betampona Nature Reserve (BNR), outlined by the white boundary in Figure 1 (a).  
															The BNR is located in the eastern coast of Madagascar; the surrounding areas and the nature reserve make 
															up of about 100 square km.  This is a species rich area which provides the researchers with a “living laboratory” 
															for the studies of human-forest interactions.<br />  
															Madagascar houses a lively and diverse ecosystem, but due to encroachment, 
															deforestation tactics (illegal logging), aggressive agricultural practices and 
															urbanization, the environment has greatly altered.  Additionally, with the presence 
															of invasive species such as Molucca Raspberry, Madagascar Cardamom, and Strawberry 
															Guava, biodiversity continues to be threatened.
														</p>

                            							<header>
															<center><h2>Data Processing</h2></center>
														</header>

														<table>
															<tr>
																<th>File Name</th>
																<th>Description</th>
															</tr>
															<tr>
																<td>Bet_LandCover_2019.tif</td>
																<td>
																	This is the “gold-standard" land cover/use map of the study area produced 
																	by object-based classification/segmentation of WV-3 imagery and significant 
																	post-processing and manual investigation using field surveys and ancillary data. 
																</td>
 
															</tr>
															<tr>
																<td>Bet_LandCover_2010.tif</td>
																<td>
																	Classification map of the BNR and surrounding areas in 2010, created using 
																	IKONOS-2 and Hyperion image analysis. <br />
																	Ghulam, A., Porton, I., Freeman, K. (2014). Detecting subcanopy invasive 
																	plant species in tropical rainforest by integrating optical and microwave 
																	(InSAR/PolInSAR) remote sensing data, and a decision tree algorithm. 
																	ISPRS Journal of Photogrammetry and Remote Sensing, 88: 174-192.
																</td>
															</tr>
															<tr>
																<td>Polygon.shp</td>
																<td>
																	This polygon represents the boundary of image patches, which will be later 
																	used to train the U-Net architecture and evaluate the results. There are a 
																	total 100 features in the polygon shapefile.
																</td>
															</tr>
															<tr>
																<td>WV3_19Feb2019_BNR.tif</td>
																<td>
																	16 band WorldView-3 imagery in reflectance including VNIR and SWIR bands, which are 
																	atmospherically and radiometrically corrected and orthorectified. <br />
																	WV-3 VNIR and SWIR ground sampling distance (GSD) is 1.2 m and 3.7 m, respectively. <br />
																	VNIR-SWIR bands are layer-stacked and resampled to VNIR resolution using nearest-neighbor resampling.<br />
																	The SWIR bands can detect non-pigment biochemical phenomenon within plants including water, cellulose, and lignin.  
																	SWIR-4 (1729.5 nm), SWIR-5 (2163.7 nm), and SWIR-8 (2329.2 nm) can identify 
																	absorption phenomenon of nitrogen, cellulose, and lignin, respectively. <br />
																	The data were obtained from MAXAR through priority satellite tasking during the experiment.  
																</td>
															</tr>
														</table>

															<p>
																The original ground truth data were collected in the field using GPS surveys. 
																A total of nearly 400 polygons in different sizes and shapes and point samples 
																were collected. These ground surveys were used to produce the “gold-standard" 
																reference imagery (Bet_LandCover_2019.tif) as described in the published paper.
																For this course module, however, we generated a new set of training samples
																from the gold-standard reference imagery to simplify the implementation.<br />

																First, observe the following map:
																<center><a href="#" class="image featured"><img src="images/four_sat_img.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />

																As stated above, the WV-3 image contains 16 bands. Further details about the 16 bands 
																can be found in  Cota et al. (2021). There are a total of 11 land cover classes that 
																were created by CNN-based image segmentation.  That means all pixels in the study area 
																already classified as some land cover/use type. The codes for each land cover class 
																are given below: <br />

																<table>
																	<tr>
																		<th>Land Use Classes</th>
																	</tr>
																	<tr>
										  								<th>Int Code</th>
										  								<th>Class</th>
																	</tr>
																	<tr>
																		<td>1</td>
																		<td>Mixed Forest</td>
																	</tr>
																	<tr>
																		<td>2</td>
																		<td>Evergreen Forest</td>
																	</tr>
																	<tr>
																		<td>3</td>
																		<td>Residential</td>
																	</tr>
																	<tr>
																		<td>4</td>
																		<td>Molucca Raspberry</td>
																	</tr>
																	<tr>
																		<td>5</td>
																		<td>Row Crops</td>
																	</tr>
																	<tr>
																		<td>6</td>
																		<td>Fallow</td>
																	</tr>
																	<tr>
																		<td>7</td>
																		<td>Shrubland</td>
																	</tr>
																	<tr>
																		<td>8</td>
																		<td>Open Water</td>
																	</tr>
																	<tr>
																		<td>9</td>
																		<td>Madagascar Cardamon</td>
																	</tr>
																	<tr>
																		<td>10</td>
																		<td>Grassland</td>
																	</tr>
																	<tr>
																		<td>11</td>
																		<td>Guava</td>
																	</tr>
																</table>
															</p>
															<p>
															U-NET requires image patches or image chips as input data. To create this data for 
															this tutorial, we generated a polygon shapefile, named ‘polygon.shp”, based on the 
															gold-standard land cover/use map. This polygon represents the boundary of image 
															patches, which will be later used to train the U-Net architecture and evaluate the 
															results. There are a total of 100 features in the polygon shapefile. The steps to 
															generate these training samples are below. <br />
															1.	Create a set of randomly generated points using ArcGIS Pro. To do this, 
															open ArcToolbox and open the Data Management 
															Tools > Feature Class > Create random points tool.<br />
																2.	Create a circular buffer around those points, set buffer size to 192 meters.<br />
															3.	Convert the circle to a square-shaped polygon by using “Minimum Bounding Geometry” 
															tool and use “Envelope” as the geometry type

															<center><a href="#" class="image featured"><img src="images/Polygons.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center><br />
														</p>
					
														<header>
															<center><h2>Objectives</h2></center>
															<p> Please go through all the provided DEMO notebooks:<br /> 
								
																•UNET_part1A_preprocessing_DEMO.ipynb<br />
																o	Demonstrate the preprocessing steps.<br />
																o	After preprocessing, save the train and test set in npy format.<br />
																•UNET_part1B_training_DEMO.ipynb<br />
																o	Perform the training of UNET.<br />
																o	Save the best model.<br />
																•UNET_part1C_applying_model_to_map_DEMO.ipynb<br />
																o	Apply the model to the whole image.<br />
																o	Divide the image into similar size image patches.<br />
																o	Then apply model to each patch.<br />
															</p>
														</header>
			
														<header>
															<center><h2>Assignment</h2></center>
															<p>	
																Write a report (no more than 4 pages) about the methods and results from this lab. 
																You can rely on the Cota et al. (2021) publication for reference, but note that the
																goal for this report is how the U-Net worked and how you implemented it. Focus on how 
																each segment of code worked and what are the 5 key skills you learned about CNN and 
																U-NET. You can include figures but do not fill up the whole report with just figures. 
																Here is a template you can follow:<br />
	
																1.	Introduction: Talk about the background behind the task. Why are you doing this? What are you expecting from the model? The objectives.<br />
																2.	Methods: (Try to create a short figure showing the overall workflow)<br />
																	a.	Preprocessing: How did you preprocess the data. Try to briefly describe the tasks of the codes provided in the notebook<br />
																	b.	U-NET Training: How many parameters were there? What hyperparameter values you used? What loss function and optimizer you used and why?<br />
																	c.	Applying to Image: How did you apply the model to the whole image?<br />
																3.	Results: What is the overall accuracy? Show the confusion matrix, kappa scores and other metrics you want to show.<br />
																4.	Discussion: is this a segmentation or classification approach? What are the advantages and disadvantages between the two? Did U-NET utilize spectral information? What about texture and spatial patterns?<br />
																5.	Conclusion: What did you achieve? What are the possible next steps that can improve the results?<br />
									
																Use Time New Roman / Arial / Calibri with 11 pt font size. Use default line 
																spacing (Multiple, 1.08). Letter page.
															</p>
														</header>
					
														<header>
															<center><h2>Grading Scale</h2></center>
														</header>
															<p>
																1.	Run all the codes correctly and generate figures in the notebook (50%)<br />
																2.	Write down the report (50% Broken down below)<br />
																	a.	Introduction (5%)<br />
                                            							i. Appropriate background and problem statement<br />
                                            							ii.Clear objective<br />
																	b.	Methods (20%)<br />
                                            							i. Clear picture about the methods<br />
                                           								ii. Explain the major functions<br />
																	c.	Results (10%)<br />
                                            							i. Evaluation metrics for test set<br />
                                            							ii. Train-validation loss curve<br />
																	d.	Discussion (10%)<br />
                                            							i. Discuss the results<br />
                                           	 							ii.Difference between segmentation and classification<br />
																	e.	Conclusion (5%)
															</p>
								
														<center><a href="#" class="image featured"><img src="images/natcapital.jpg" style= "width: 600px; height:auto" display="block" alt="" class="center" /></a></center>
													</p>
												</details>
											</div>
										</section>
										

									<!-- Lets Talk About Levee's! -->
										<section id="levee" class="two">
											<div class="container">
												<details>
													<summary>
									  					<header>
										  					<center><h2>Water Disparity and Levee Management</h2></center>
									  					</header>
									  				<span class="icon">↓</span>
													</summary>
													<!--Everything about module needs to go in between these two p's for dropdown!-->
													<header>
														<h3><a href="https://github.com/MissouriView/Water-Disparity-and-Levee-Management" target= "_blank">Link</a> to Github page</h3>
													</header>
													<p> 
														This project uses LiDAR and Photogrammetry data for levee managmeent
														An essential aspect of large rivers is the human need to control them. Installing 
														levees is one way that society has tried to control flooding on large rivers. As levees 
														age, they need to be managed and upkept. In areas where levees are managed by different groups, 
														instances of inconsistent management can sometimes lead to nearby levees being updated while others 
														are forgotten. One case of inconsistent management, and a subsequent levee failure, is the Len 
														Small Levee on the Mississippi River, just north of it’s confluence with the Ohio River in a stretch 
														of river known as the Dogtooth Bend. The Len Small Levee is a farmer-owned levee currently being eroded
														by the Mississippi River as it forms a chute across farm lands. Now, residents living in the area must 
														travel by boat if they wish to leave the newly formed island.<br />
									
														This training module is forthcoming!
													</p>
												</details>
											</div>
										</section>

										<!--MO as Art Tutorial Spot-->	
										<section id="art" class="two">
											<div class="container">
												<details>
													<summary>
														<header>
															<center><h2>Missouri as Art</h2></center>
														</header>
														<span class="icon">↓</span>
														</summary>
														<!--Everything about module needs to go in between these two p's for dropdown!-->
														<header>
															<h3><a href="https://github.com/MissouriView/Missouri-as-Art" target= "_blank">Link</a> to Github page</h3>
														</header>
														<p>
															Missouri is a naturally beautiful state, and one of 
															the most stunning ways to visualize Missouri’s beauty is by using different 
															multispectral band combinations to optimize the visualization of remote sensing data 
															to highlight different features on the ground. The Missouri as Art portion of MissouriView’s 
															mission makes remote sensing data more visually intriguing and artistic for building interest 
															from K-12 communities, while introducing students to applied concepts in remote sensing. 
															Here, students are introduced to the Electromagnetic Spectrum, the concepts of remote sensing 
															bands, how different bands are visualized, and how different combinations of bands can 
															highlight features of different types (i.e. water, agriculture, mineral exploration and 
															extraction, etc...).Power point material provides introduction to remote sensing with ample 
															examples of imagery collected in Missouri. With intuitive combination of satellite imagery, 
															the material highlight distinct Missouri landscape, ideal training material for K-16. <br />

															Tutorial Coming Soon!
														</p>
												</details>
											</div>
										</section>
		
				<!--Potential Holding place-->

				
			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; SLU RS Lab. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
